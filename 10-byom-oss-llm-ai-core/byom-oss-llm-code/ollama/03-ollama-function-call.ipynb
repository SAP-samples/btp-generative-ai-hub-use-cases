{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0a5a74d",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "In this notebook, we will test out the function calling with Meta's [llama 3.1](https://ollama.com/library/llama3.1) or Mistral's [mistral v0.3](https://ollama.com/library/mistral) served with [Ollama](https://ollama.com/) in SAP AI Core.  <br/><br/>\n",
    "\n",
    "Please refer to this [blog post](https://community.sap.com/t5/artificial-intelligence-and-machine-learning-blogs/from-unstructured-input-to-structured-output-llm-meets-sap/ba-p/13772506) about custom function calling with open-source models through Ollama in SAP AI Core for more details.\n",
    "\n",
    "### Prerequisites\n",
    "Before running this notebook, please assure you have performed the [Prerequisites](../../README.md) and [01-deployment.ipynb](01-deployment.ipynb). As a result, a deployment of Ollama scenario is running in SAP AI Core. <br/><br/>\n",
    "\n",
    "If the configuration and deployment are created through SAP AI Launchpad, please manually update the configuration_id and deployment_id in [env.json](env.json)\n",
    "```json\n",
    "{\n",
    "    \"configuration_id\": \"<YOUR_CONFIGURATION_ID_OF_OLLAMA_SCENARIO>\",\n",
    "    \"deployment_id\": \"<YOUR_DEPLOYMENT_ID_BASED_ON_CONFIG_ABOVE>\"\n",
    "}\n",
    "```\n",
    " \n",
    "### The high-level flow:\n",
    "- Load configurations info\n",
    "- Connect to SAP AI Core via SDK\n",
    "- Check the status and logs of the deployment\n",
    "- Pull model from ollama model repository through API\n",
    "- Inference the model with OpenAI-compatible chat completion API for function call\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c55bd7b",
   "metadata": {},
   "source": [
    "#### 1.Load config info \n",
    "- resource_group loaded from [config.json](../config.json)\n",
    "- deployment_id(created in 01-deployment.ipynb) loaded [env.json](env.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90f1e9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "from ai_api_client_sdk.ai_api_v2_client import AIAPIV2Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5eee26b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment id:  df240ccfb2d899b0  resource group:  oss-llm\n"
     ]
    }
   ],
   "source": [
    "# Please replace the configurations below.\n",
    "# config_id: The target configuration to create the deployment. Please create the configuration first.\n",
    "with open(\"../config.json\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "with open(\"./env.json\") as f:\n",
    "    env = json.load(f)\n",
    "\n",
    "deployment_id = env[\"deployment_id\"]\n",
    "resource_group = config.get(\"resource_group\", \"default\")\n",
    "print(\"deployment id: \", deployment_id, \" resource group: \", resource_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd694c3",
   "metadata": {},
   "source": [
    "#### 2.Initiate connection to SAP AI Core "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a4cc0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "aic_sk = config[\"ai_core_service_key\"]\n",
    "base_url = aic_sk[\"serviceurls\"][\"AI_API_URL\"] + \"/v2/lm\"\n",
    "ai_api_client = AIAPIV2Client(\n",
    "    base_url= base_url,\n",
    "    auth_url=aic_sk[\"url\"] + \"/oauth/token\",\n",
    "    client_id=aic_sk['clientid'],\n",
    "    client_secret=aic_sk['clientsecret'],\n",
    "    resource_group=resource_group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ffb297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = ai_api_client.rest_client.get_token()\n",
    "headers = {\n",
    "        \"Authorization\": token,\n",
    "        'ai-resource-group': resource_group,\n",
    "        \"Content-Type\": \"application/json\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d7b416",
   "metadata": {},
   "source": [
    "#### 3.Check the deployment status "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d46cf76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment-df240ccfb2d899b0 is running. Ready for inference request\n"
     ]
    }
   ],
   "source": [
    "# Check deployment status before inference request\n",
    "deployment_url = f\"{base_url}/deployments/{deployment_id}\"\n",
    "response = requests.get(url=deployment_url, headers=headers)\n",
    "resp = response.json()    \n",
    "status = resp['status']\n",
    "\n",
    "deployment_log_url = f\"{base_url}/deployments/{deployment_id}/logs\"\n",
    "if status == \"RUNNING\":\n",
    "        print(f\"Deployment-{deployment_id} is running. Ready for inference request\")\n",
    "else:\n",
    "        print(f\"Deployment-{deployment_id} status: {status}. Not yet ready for inference request\")\n",
    "        #retrieve deployment logs\n",
    "        #{{apiurl}}/v2/lm/deployments/{{deploymentid}}/logs.\n",
    "\n",
    "        response = requests.get(deployment_log_url, headers=headers)\n",
    "        print('Deployment Logs:\\n', response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847b4fcb",
   "metadata": {},
   "source": [
    "#### 4.Pull the model into Ollama "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d86047d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model support function calling: llama3.1 and mistral\n",
    "model = \"llama3.1\"\n",
    "#model = \"mistral\"\n",
    "\n",
    "deployment = ai_api_client.deployment.get(deployment_id)\n",
    "inference_base_url = f\"{deployment.deployment_url}/v1\"\n",
    "openai_base_url = deployment.deployment_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc07e607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull the model from ollama model repository\n",
    "endpoint = f\"{inference_base_url}/api/pull\"\n",
    "print(endpoint)\n",
    "\n",
    "#let's pull the mistral model from ollama\n",
    "json_data = {  \"name\": model}\n",
    "\n",
    "response = requests.post(endpoint, headers=headers, json=json_data)\n",
    "print('Result:', response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefa026a",
   "metadata": {},
   "source": [
    "Next, let's list the model and check if the target model is listed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff40e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the model list \n",
    "endpoint = f\"{inference_base_url}/api/tags\"\n",
    "print(endpoint)\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "print('Result:', response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a7d13c",
   "metadata": {},
   "source": [
    "#### 5.Invoke custom Function Call with chat completion APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c0658246",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_api_endpoint = f\"{inference_base_url}/api/generate\"\n",
    "chat_api_endpoint = f\"{inference_base_url}/api/chat\"\n",
    "openai_chat_api_endpoint = f\"{openai_base_url}/v1/chat/completions\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8848db40",
   "metadata": {},
   "source": [
    "##### 5.1 Sample#1: Get Current Weather with chat completion API\n",
    "Let's test it with a basic sample of function with [ollama's chat completion API](https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-chat-completion) for answering the question \"What is the weather today in Melbourne, Australia?\".\n",
    "![Process flow of function calling for getting current weather](../../resources/20-function-call-flow-get-weather.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b636db2b",
   "metadata": {},
   "source": [
    "Step 1: Function Call to LLM with the initial question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb1b4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the weather today in Melbourne, Australia?\"\n",
    "json_data = {\n",
    "  \"model\": model,\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": question\n",
    "    }\n",
    "  ],\n",
    "  \"stream\": False,\n",
    "  \"format\": \"json\", #enable JSON mode to assure valid json response for function call\n",
    "  \"tools\": [\n",
    "    {\n",
    "      \"type\": \"function\",\n",
    "      \"function\": {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather for a location\",\n",
    "        \"parameters\": {\n",
    "          \"type\": \"object\",\n",
    "          \"properties\": {\n",
    "            \"location\": {\n",
    "              \"type\": \"string\",\n",
    "              \"description\": \"The location to get the weather for, e.g. San Francisco, CA\"\n",
    "            },\n",
    "            \"format\": {\n",
    "              \"type\": \"string\",\n",
    "              \"description\": \"The format to return the weather in, e.g. 'celsius' or 'fahrenheit'\",\n",
    "              \"enum\": [\"celsius\", \"fahrenheit\"]\n",
    "            }\n",
    "          },\n",
    "          \"required\": [\"location\", \"format\"]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "response = requests.post(url=chat_api_endpoint, headers=headers, json=json_data)\n",
    "print('Result:', response.text)\n",
    "\n",
    "# parse the json response to retrieve the location and format, to be passed to 3rd-party weather API\n",
    "resp_json = response.json()\n",
    "func_dict = resp_json['message']['tool_calls'][0]['function']\n",
    "func = func_dict['name']\n",
    "args_dict = func_dict['arguments']\n",
    "location = args_dict['location']\n",
    "format = args_dict['format']\n",
    "\n",
    "print('Function:', func)\n",
    "print('Location:', location)\n",
    "print('Format:', format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466123fd",
   "metadata": {},
   "source": [
    "Step 2: Service Fulfilment to the 3rd-party weather API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07234248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# service fulfillment by 3rd-party API with given location and format... for example, let's assume the 3rd party API returns a json weather condition like this. we'll instruct llama3.1 to answer the question with this service response\n",
    "def get_current_weather(location, format):\n",
    "    # Your actual API call goes here...\n",
    "    response = { \"condition\": \"Rainy\", \"temp_h\": 15, \"temp_l\": 7, \"temp_unit\": \"C\" }\n",
    "    return response\n",
    "service_resp = get_current_weather(location, format)\n",
    "service_resp_str = json.dumps(service_resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ff402b",
   "metadata": {},
   "source": [
    "Step 3: Generate answer to the original question with the service response as context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b21c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answering the original question with the service response as context\n",
    "user_msg = \"\"\"\n",
    "context: {}\n",
    "\n",
    "Answer the question with context(weather API response in json) above including weather condition as emoji and temperatures range: {}?Be concise.\n",
    "\"\"\".format(service_resp_str,question)\n",
    "\n",
    "json_data = {\n",
    "  \"model\": model,\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": user_msg\n",
    "    }\n",
    "  ],\n",
    "  \"stream\": False\n",
    "}\n",
    "\n",
    "response = requests.post(url=chat_api_endpoint, headers=headers, json=json_data)\n",
    "resp_json = response.json()\n",
    "print('Final Response JSON:', resp_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c1a7bd",
   "metadata": {},
   "source": [
    "##### 5.2 Sample#1: Get Current Weather with OpenAI-compatible Chat Completion API\n",
    "Let's test it with a basic sample with [Ollama's OpenAI-compatible Chat Completion API](https://github.com/ollama/ollama/blob/main/docs/openai.md) of function for answering the question \"What is the weather today in Melbourne, Australia?\". The function call body here is the same as ollama's chat completion API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49a00d6",
   "metadata": {},
   "source": [
    "Step 1: Function Call to LLM with the initial question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95d58be",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the weather today in Melbourne, Australia?\"\n",
    "\n",
    "json_data = {\n",
    "  \"model\": model,\n",
    "  \"stream\": False, # Streaming mode is to be supported\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": question\n",
    "    }\n",
    "  ],\n",
    "  \"tools\": [\n",
    "    {\n",
    "      \"type\": \"function\",\n",
    "      \"function\": {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"parameters\": {\n",
    "          \"type\": \"object\",\n",
    "          \"title\": \"weather parameters\",\n",
    "          \"properties\": {\n",
    "            \"location\": {\n",
    "              \"type\": \"string\",\n",
    "              \"description\": \"The location to get the weather for, e.g. San Francisco, CA\"\n",
    "            },\n",
    "            \"format\": {\n",
    "              \"type\": \"string\",\n",
    "              \"description\": \"The format to return the weather in, e.g. 'celsius' or 'fahrenheit'\",\n",
    "              \"enum\": [\"celsius\", \"fahrenheit\"]\n",
    "            }\n",
    "          },\n",
    "          \"required\": [\"location\", \"format\"]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  # as of 13 Aug, tool_choice not yet supported by ollama's openai-compatible api\n",
    "  \"tool_choice\": [\n",
    "    {\n",
    "      \"type\": \"function\",\n",
    "      \"function\": {\n",
    "        \"name\": \"get_current_weather\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "response = requests.post(url=openai_chat_api_endpoint, headers=headers, json=json_data)\n",
    "print('Result:', response.text)\n",
    "\n",
    "# parse the json response to retrieve the location and format, to be passed to 3rd-party weather API\n",
    "resp_json = response.json()\n",
    "func_dict = resp_json['choices'][0]['message']['tool_calls'][0]['function']\n",
    "func = func_dict['name']\n",
    "args_dict = json.loads(func_dict['arguments']) #arguments are returned as a json string, so we need to parse it.\n",
    "location = args_dict['location']\n",
    "format = args_dict['format']\n",
    "\n",
    "print('Function:', func)\n",
    "print('Location:', location)\n",
    "print('Format:', format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52595156",
   "metadata": {},
   "source": [
    "Step 2: Service Fulfilment to the 3rd-party weather API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d0fc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# service fulfillment by 3rd-party API with given location and format... for example, let's assume the 3rd party API returns a json weather condition like this. we'll instruct llama3.1 to answer the question with this service response\n",
    "def get_current_weather2(location, format):\n",
    "    # Your actual API call goes here...\n",
    "    response = { \"condition\": \"Rainy\", \"temp_h\": 15, \"temp_l\": 7, \"temp_unit\": \"C\" }\n",
    "    return response\n",
    "service_resp = get_current_weather2(location, format)\n",
    "service_resp_str = json.dumps(service_resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488893d8",
   "metadata": {},
   "source": [
    "Step 3: Generate answer to the original question with the service response as context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf8a184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answering the original question with the service response as context\n",
    "user_msg = \"\"\"\n",
    "context: {}\n",
    "\n",
    "Answer the question with context(weather API response in json) above including weather condition as emoji and temperatures range: {}?Be concise.\n",
    "\"\"\".format(service_resp_str,question)\n",
    "\n",
    "json_data = {\n",
    "  \"model\": model,\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": user_msg\n",
    "    }\n",
    "  ],\n",
    "  \"stream\": False\n",
    "}\n",
    "\n",
    "response = requests.post(url=openai_chat_api_endpoint, headers=headers, json=json_data)\n",
    "resp_json = response.json()\n",
    "print('Final Response JSON:', resp_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
